{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook, I will proceed with the pickled data and start the iterative machine learning process (conceptualize -> code -> experiment)\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "# Technology used: Tensorflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, I start with the utility cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# packages used for processing:\n",
    "import cPickle as pickle # for pickling the processed data\n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import numpy as np # numerical computations\n",
    "\n",
    "# for operating system related stuff\n",
    "import os\n",
    "import sys # for memory usage of objects\n",
    "from subprocess import check_output\n",
    "\n",
    "# the boss of tensorflow frameworks\n",
    "import tensorflow as tf\n",
    "\n",
    "# to plot the images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply the seaborn makeup on the plots drawn using matplotlib\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "\n",
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print(check_output(cmd).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "LICENSE\n",
      "Models\n",
      "README.md\n",
      "Scripts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the structure of the project directory\n",
    "exec_command(['ls', '..'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Set the constants for the script '''\n",
    "\n",
    "# various paths of the files\n",
    "data_path = \"../Data\" # the data path\n",
    "base_model_path = \"../Models\"\n",
    "\n",
    "data_files = {\n",
    "    \"train\": os.path.join(data_path, \"train.csv\"),\n",
    "    \"test\": os.path.join(data_path, \"test.csv\")\n",
    "}\n",
    "\n",
    "base_model_path = '../Models'\n",
    "\n",
    "plug_and_play_data_file_path = os.path.join(data_path, \"plug_and_play.pickle\")\n",
    "\n",
    "# constants:\n",
    "(train_size, dev_size, test_size) = (0.9, 0.05, 0.05) # values are unit ratios\n",
    "no_of_features = 57\n",
    "no_of_itreations = 10000 \n",
    "batch_size = 512\n",
    "checkpoint_factor = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to unpickle the given file and load the obj back into the python environment\n",
    "def unPickleIt(pickle_path): # might throw the file not found exception\n",
    "    '''\n",
    "        function to unpickle the object from the given path\n",
    "        @param\n",
    "        pickle_path => the path where the pickle file is located\n",
    "        @return => the object extracted from the saved path\n",
    "    '''\n",
    "\n",
    "    with open(pickle_path, 'rb') as dumped_pickle:\n",
    "        obj = pickle.load(dumped_pickle)\n",
    "\n",
    "    return obj # return the unpickled object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the data and create the train / dev / test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dict = unPickleIt(plug_and_play_data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data_dict['features']; Y = data_dict['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((57, 595212), (1, 595212))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape # check if the shapes are compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep the means and variances for the features\n",
    "means = data_dict['means']; variances = data_dict['variances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to split the data into train, dev and test sets\n",
    "def train_dev_test_split_data(X, Y):\n",
    "    '''\n",
    "        function to split the X and Y arrays into train, dev and test sets\n",
    "        @param\n",
    "        X => the input features to train on\n",
    "        Y => the ideal labels for the given inputs\n",
    "        @return => train_X, train_Y, dev_X, dev_Y, test_X, test_Y: the names suggest meanings\n",
    "    '''\n",
    "    m_examples = X.shape[-1] # total number of examples to train on\n",
    "    \n",
    "    # first parition point\n",
    "    train_dev_partition_point = int((m_examples * train_size) + 0.5)\n",
    "    \n",
    "    # second partition point \n",
    "    dev_test_partition_point = train_dev_partition_point + int((m_examples * dev_size) + 0.5)\n",
    "    \n",
    "    ''' perform the actual split of the data '''\n",
    "    # Training set splitting:\n",
    "    train_X = X[:, : train_dev_partition_point]; train_Y = Y[:, : train_dev_partition_point]\n",
    "    \n",
    "    # dev set splitting\n",
    "    dev_X = X[:, train_dev_partition_point: dev_test_partition_point]\n",
    "    dev_Y = Y[:, train_dev_partition_point: dev_test_partition_point]\n",
    "    \n",
    "    # test set splitting\n",
    "    test_X = X[:, dev_test_partition_point:]; test_Y = Y[:, dev_test_partition_point:]\n",
    "    \n",
    "    # return the so formed splits\n",
    "    return train_X, train_Y, dev_X, dev_Y, test_X, test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, train_Y, dev_X, dev_Y, test_X, test_Y = train_dev_test_split_data(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training X shape: (57, 535691)\n",
      "Training Y shape: (1, 535691)\n",
      "Dev X shape     : (57, 29761)\n",
      "Dev Y shape     : (1, 29761)\n",
      "Test X shape    : (57, 29760)\n",
      "Test Y shape    : (1, 29760)\n"
     ]
    }
   ],
   "source": [
    "# print the shapes of all the above obtained sets:\n",
    "print \"Training X shape: \" + str(train_X.shape)\n",
    "print \"Training Y shape: \" + str(train_Y.shape)\n",
    "print \"Dev X shape     : \" + str(dev_X.shape)\n",
    "print \"Dev Y shape     : \" + str(dev_Y.shape)\n",
    "print \"Test X shape    : \" + str(test_X.shape)\n",
    "print \"Test Y shape    : \" + str(test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both the assertions pass!!\n"
     ]
    }
   ],
   "source": [
    "# Make sure that no Example has been left out\n",
    "assert X.shape[-1] == np.hstack((train_X, dev_X, test_X)).shape[-1], \"Examples have been left out\"\n",
    "assert Y.shape[-1] == np.hstack((train_Y, dev_Y, test_Y)).shape[-1], \"Labels have been left out\"\n",
    "\n",
    "# If both the above asserts are successful, we can go ahead and print the following statement\n",
    "print \"Both the assertions pass!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cool! So now Let's get onto the part where we build the Tensorflow Graph\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "## I am going to keep the graph scoped and in a single cell, so that I can port it into the production graph file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_dims = [512, 512, 512, 256, 1] # the num_units in each layer of the feed_forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 535691)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the tensorflow computation graph (THE MAIN NEURAL NETWORK):\n",
    "\n",
    "model1 = tf.Graph()\n",
    "\n",
    "with model1.as_default():\n",
    "    # scoped as Inputs\n",
    "    with tf.variable_scope(\"Input\"):\n",
    "        \n",
    "        # define the placeholders for the input data\n",
    "        input_X = tf.placeholder(tf.float32, shape=(None, no_of_features), name=\"Input_features\") # placeholder for feeding in input data batch\n",
    "        labels_Y = tf.placeholder(tf.float32, shape=(None, 1), name=\"Ideal_labels\") # placeholder for the labels\n",
    "    \n",
    "    # scoped as model:\n",
    "    with tf.variable_scope(\"Deep_Neural_Network\"):\n",
    "        \n",
    "        # define the layers for the neural network.\n",
    "        ''' This is a plain and simple neural network with relu activations '''\n",
    "        # layer 1 => \n",
    "        lay1 = tf.layers.dense(input_X, layer_dims[0], activation=tf.nn.relu, name=\"layer_1\")\n",
    "        # layer 2 =>\n",
    "        lay2 = tf.layers.dense(lay1, layer_dims[1], activation=tf.nn.relu, name=\"layer_2\")\n",
    "        # layer 3 =>\n",
    "        lay3 = tf.layers.dense(lay2, layer_dims[2], activation=tf.nn.relu, name=\"layer_3\")\n",
    "        # layer 4 =>\n",
    "        lay4 = tf.layers.dense(lay3, layer_dims[3], activation=tf.nn.relu, name=\"layer_4\")\n",
    "        # layer 5 =>\n",
    "        # the last layer has activation sigmoid since it is going to output probability.\n",
    "        lay5 = tf.layers.dense(lay4, layer_dims[4], name=\"output\") # the activation is linear\n",
    "        \n",
    "        \n",
    "        ''' Separately record all the activations as histograms '''\n",
    "        # recording the summaries to visualize separately\n",
    "        lay1_summary = tf.summary.histogram(\"lay1_summary\", lay1)\n",
    "        lay2_summary = tf.summary.histogram(\"lay2_summary\", lay2)\n",
    "        lay3_summary = tf.summary.histogram(\"lay3_summary\", lay3)\n",
    "        lay4_summary = tf.summary.histogram(\"lay4_summary\", lay4)\n",
    "        output_summary = tf.summary.histogram(\"output_summary\", lay5)\n",
    "        \n",
    "    # scoped as predictions\n",
    "    with tf.variable_scope(\"Prediction\"):\n",
    "        prediction = tf.nn.sigmoid(lay5, name=\"sigmoid\") # apply sigmoid to the linear activation of the output\n",
    "        \n",
    "    # scoped as loss\n",
    "    with tf.variable_scope(\"Loss\"):\n",
    "        \n",
    "        # define the loss function.\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=lay5, labels=labels_Y), name=\"loss\")\n",
    "        # we use the softmanx_cross_entropy_with_logits function for this.\n",
    "        \n",
    "        # record the loss summary:\n",
    "        tf.summary.scalar(\"Loss\", loss)\n",
    "        \n",
    "    # scoped as train_step\n",
    "    with tf.variable_scope(\"Train_Step\"):\n",
    "    \n",
    "        # define the optimizer and the train_step:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=1e-6) # use the default learning rate\n",
    "        train_step = optimizer.minimize(loss, name=\"train_step\")\n",
    "        \n",
    "    # scoped as init operation\n",
    "    with tf.variable_scope(\"Init\"):\n",
    "        init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    # scoped as summaries\n",
    "    with tf.variable_scope(\"Summary\"):\n",
    "        all_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The graph has been defined. Now, use the session executer to run the graph and see how it trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = \"Model1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to execute the session and train the model:\n",
    "def execute_graph(dataX, dataY, exec_graph, model_name, no_of_iterations):\n",
    "    '''\n",
    "        function to start and execute the session with training.\n",
    "        @param \n",
    "        dataX, dataY => the data to train on\n",
    "        exec_graph => the computation graph to be trained\n",
    "        model_name => the name of the model where the files will be saved\n",
    "        no_of_itreations => no of iterations for which the model needs to be trained\n",
    "        @return => Nothing, this function has a side effect\n",
    "    '''\n",
    "    assert dataX.shape[-1] == dataY.shape[-1], \"The Dimensions of input X and labels Y don't match\"\n",
    "    \n",
    "    # the number of examples in the dataset\n",
    "    no_of_examples = dataX.shape[-1]\n",
    "    \n",
    "    with tf.Session(graph=exec_graph) as sess:\n",
    "        # create the tensorboard writer for collecting summaries:\n",
    "        log_dir = os.path.join(base_model_path, model_name)\n",
    "        tensorboard_writer = tf.summary.FileWriter(logdir=log_dir, graph=sess.graph, filename_suffix=\".bot\")\n",
    "        \n",
    "        # The saver object for saving and loading the model\n",
    "        saver = tf.train.Saver(max_to_keep=2)\n",
    "        \n",
    "        # check if the model has been saved.\n",
    "        model_path = log_dir\n",
    "        model_file = os.path.join(model_path, model_name) # the name of the model is same as dir\n",
    "        if(os.path.isfile(os.path.join(base_model_path, model_name, \"checkpoint\"))):\n",
    "            # the model exists and you can restore the weights\n",
    "            saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "        else:\n",
    "            # no saved model found. so, run the global variables initializer:\n",
    "            sess.run(init_op)\n",
    "\n",
    "        print \"Starting the training ...\"\n",
    "        print \"===============================================================================================\"\n",
    "        \n",
    "        batch_index = 0 # initialize it to 0\n",
    "        # start the training:\n",
    "        for iteration in range(no_of_itreations):\n",
    "            \n",
    "            # fetch the input and create the batch:\n",
    "            start = batch_index; end = start + batch_size\n",
    "            inp_X = dataX[:, start: end].T # extract the input features\n",
    "            inp_Y = dataY[:, start: end].T # extract the labels\n",
    "            \n",
    "            # feed the input to the graph and get the output:\n",
    "            _, cost = sess.run((train_step, loss), feed_dict={input_X: inp_X, labels_Y: inp_Y})\n",
    "            \n",
    "            # checkpoint the model at certain times\n",
    "            if((iteration + 1) % checkpoint_factor == 0):\n",
    "                # compute the summary:\n",
    "                summary = sess.run(all_summaries, feed_dict={input_X: inp_X, labels_Y: inp_Y})\n",
    "                \n",
    "                # accumulate the summary\n",
    "                tensorboard_writer.add_summary(summary, (iteration + 1))\n",
    "                \n",
    "                # print the cost at this point\n",
    "                print \"Iteration: \" + str(iteration + 1) + \" Current cost: \" + str(cost)\n",
    "                \n",
    "                # save the model trained so far:\n",
    "                saver.save(sess, model_file, global_step = (iteration + 1))\n",
    "                \n",
    "            # increment the batch_index\n",
    "            batch_index = (batch_index + batch_size) % no_of_examples\n",
    "            \n",
    "        print \"===============================================================================================\"\n",
    "        print \"Training complete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the training ...\n",
      "===============================================================================================\n",
      "Iteration: 50 Current cost: 0.848867\n",
      "Iteration: 100 Current cost: 0.722478\n",
      "Iteration: 150 Current cost: 0.590394\n",
      "Iteration: 200 Current cost: 0.533269\n",
      "Iteration: 250 Current cost: 0.472457\n",
      "Iteration: 300 Current cost: 0.448901\n",
      "Iteration: 350 Current cost: 0.414152\n",
      "Iteration: 400 Current cost: 0.362162\n",
      "Iteration: 450 Current cost: 0.360992\n",
      "Iteration: 500 Current cost: 0.328143\n",
      "Iteration: 550 Current cost: 0.316141\n",
      "Iteration: 600 Current cost: 0.288144\n",
      "Iteration: 650 Current cost: 0.275934\n",
      "Iteration: 700 Current cost: 0.332201\n",
      "Iteration: 750 Current cost: 0.323909\n",
      "Iteration: 800 Current cost: 0.214473\n",
      "Iteration: 850 Current cost: 0.215616\n",
      "Iteration: 900 Current cost: 0.229044\n",
      "Iteration: 950 Current cost: 0.217725\n",
      "Iteration: 1000 Current cost: 0.231152\n",
      "Iteration: 1050 Current cost: 0.236267\n",
      "Iteration: 1100 Current cost: 0.217161\n",
      "Iteration: 1150 Current cost: 0.203768\n",
      "Iteration: 1200 Current cost: 0.187398\n",
      "Iteration: 1250 Current cost: 0.244081\n",
      "Iteration: 1300 Current cost: 0.168769\n",
      "Iteration: 1350 Current cost: 0.18945\n",
      "Iteration: 1400 Current cost: 0.216996\n",
      "Iteration: 1450 Current cost: 0.212805\n",
      "Iteration: 1500 Current cost: 0.18393\n",
      "Iteration: 1550 Current cost: 0.203305\n",
      "Iteration: 1600 Current cost: 0.146181\n",
      "Iteration: 1650 Current cost: 0.219885\n",
      "Iteration: 1700 Current cost: 0.152141\n",
      "Iteration: 1750 Current cost: 0.153245\n",
      "Iteration: 1800 Current cost: 0.232424\n",
      "Iteration: 1850 Current cost: 0.228683\n",
      "Iteration: 1900 Current cost: 0.198176\n",
      "Iteration: 1950 Current cost: 0.20673\n",
      "Iteration: 2000 Current cost: 0.313079\n",
      "Iteration: 2050 Current cost: 0.318279\n",
      "Iteration: 2100 Current cost: 0.179385\n",
      "Iteration: 2150 Current cost: 0.173353\n",
      "Iteration: 2200 Current cost: 0.168694\n",
      "Iteration: 2250 Current cost: 0.192676\n",
      "Iteration: 2300 Current cost: 0.195913\n",
      "Iteration: 2350 Current cost: 0.195072\n",
      "Iteration: 2400 Current cost: 0.198401\n",
      "Iteration: 2450 Current cost: 0.193873\n",
      "Iteration: 2500 Current cost: 0.208642\n",
      "Iteration: 2550 Current cost: 0.17333\n",
      "Iteration: 2600 Current cost: 0.192424\n",
      "Iteration: 2650 Current cost: 0.196603\n",
      "Iteration: 2700 Current cost: 0.175193\n",
      "Iteration: 2750 Current cost: 0.174041\n",
      "Iteration: 2800 Current cost: 0.234202\n",
      "Iteration: 2850 Current cost: 0.186869\n",
      "Iteration: 2900 Current cost: 0.18824\n",
      "Iteration: 2950 Current cost: 0.148386\n",
      "Iteration: 3000 Current cost: 0.129362\n",
      "Iteration: 3050 Current cost: 0.129366\n",
      "Iteration: 3100 Current cost: 0.25112\n",
      "Iteration: 3150 Current cost: 0.173357\n",
      "Iteration: 3200 Current cost: 0.18613\n",
      "Iteration: 3250 Current cost: 0.146495\n",
      "Iteration: 3300 Current cost: 0.189935\n",
      "Iteration: 3350 Current cost: 0.151312\n",
      "Iteration: 3400 Current cost: 0.212994\n",
      "Iteration: 3450 Current cost: 0.192067\n",
      "Iteration: 3500 Current cost: 0.147103\n",
      "Iteration: 3550 Current cost: 0.144533\n",
      "Iteration: 3600 Current cost: 0.20052\n",
      "Iteration: 3650 Current cost: 0.190022\n",
      "Iteration: 3700 Current cost: 0.161681\n",
      "Iteration: 3750 Current cost: 0.170646\n",
      "Iteration: 3800 Current cost: 0.161467\n",
      "Iteration: 3850 Current cost: 0.169658\n",
      "Iteration: 3900 Current cost: 0.204455\n",
      "Iteration: 3950 Current cost: 0.186236\n",
      "Iteration: 4000 Current cost: 0.205092\n",
      "Iteration: 4050 Current cost: 0.145687\n",
      "Iteration: 4100 Current cost: 0.170657\n",
      "Iteration: 4150 Current cost: 0.133687\n",
      "Iteration: 4200 Current cost: 0.152533\n",
      "Iteration: 4250 Current cost: 0.161838\n",
      "Iteration: 4300 Current cost: 0.148071\n",
      "Iteration: 4350 Current cost: 0.151437\n",
      "Iteration: 4400 Current cost: 0.204762\n",
      "Iteration: 4450 Current cost: 0.165067\n",
      "Iteration: 4500 Current cost: 0.141479\n",
      "Iteration: 4550 Current cost: 0.160267\n",
      "Iteration: 4600 Current cost: 0.176004\n",
      "Iteration: 4650 Current cost: 0.190517\n",
      "Iteration: 4700 Current cost: 0.115459\n",
      "Iteration: 4750 Current cost: 0.178043\n",
      "Iteration: 4800 Current cost: 0.160538\n",
      "Iteration: 4850 Current cost: 0.119713\n",
      "Iteration: 4900 Current cost: 0.133839\n",
      "Iteration: 4950 Current cost: 0.105838\n",
      "Iteration: 5000 Current cost: 0.156829\n",
      "Iteration: 5050 Current cost: 0.135684\n",
      "Iteration: 5100 Current cost: 0.152147\n",
      "Iteration: 5150 Current cost: 0.11864\n",
      "Iteration: 5200 Current cost: 0.152319\n",
      "Iteration: 5250 Current cost: 0.173374\n",
      "Iteration: 5300 Current cost: 0.180486\n",
      "Iteration: 5350 Current cost: 0.174006\n",
      "Iteration: 5400 Current cost: 0.208655\n",
      "Iteration: 5450 Current cost: 0.198096\n",
      "Iteration: 5500 Current cost: 0.191069\n",
      "Iteration: 5550 Current cost: 0.133828\n",
      "Iteration: 5600 Current cost: 0.140572\n",
      "Iteration: 5650 Current cost: 0.164651\n",
      "Iteration: 5700 Current cost: 0.141217\n",
      "Iteration: 5750 Current cost: 0.151855\n",
      "Iteration: 5800 Current cost: 0.124845\n",
      "Iteration: 5850 Current cost: 0.136923\n",
      "Iteration: 5900 Current cost: 0.141998\n",
      "Iteration: 5950 Current cost: 0.167611\n",
      "Iteration: 6000 Current cost: 0.149465\n",
      "Iteration: 6050 Current cost: 0.223729\n",
      "Iteration: 6100 Current cost: 0.13183\n",
      "Iteration: 6150 Current cost: 0.14087\n",
      "Iteration: 6200 Current cost: 0.154471\n",
      "Iteration: 6250 Current cost: 0.121551\n",
      "Iteration: 6300 Current cost: 0.123089\n",
      "Iteration: 6350 Current cost: 0.118382\n",
      "Iteration: 6400 Current cost: 0.187186\n",
      "Iteration: 6450 Current cost: 0.163461\n",
      "Iteration: 6500 Current cost: 0.18677\n",
      "Iteration: 6550 Current cost: 0.133444\n",
      "Iteration: 6600 Current cost: 0.151067\n",
      "Iteration: 6650 Current cost: 0.18786\n",
      "Iteration: 6700 Current cost: 0.170804\n",
      "Iteration: 6750 Current cost: 0.116906\n",
      "Iteration: 6800 Current cost: 0.195604\n",
      "Iteration: 6850 Current cost: 0.138229\n",
      "Iteration: 6900 Current cost: 0.136698\n",
      "Iteration: 6950 Current cost: 0.234511\n",
      "Iteration: 7000 Current cost: 0.184456\n",
      "Iteration: 7050 Current cost: 0.128513\n",
      "Iteration: 7100 Current cost: 0.189954\n",
      "Iteration: 7150 Current cost: 0.203401\n",
      "Iteration: 7200 Current cost: 0.183564\n",
      "Iteration: 7250 Current cost: 0.187512\n",
      "Iteration: 7300 Current cost: 0.152466\n",
      "Iteration: 7350 Current cost: 0.192768\n",
      "Iteration: 7400 Current cost: 0.162282\n",
      "Iteration: 7450 Current cost: 0.160547\n",
      "Iteration: 7500 Current cost: 0.163008\n",
      "Iteration: 7550 Current cost: 0.126295\n",
      "Iteration: 7600 Current cost: 0.136838\n",
      "Iteration: 7650 Current cost: 0.120299\n",
      "Iteration: 7700 Current cost: 0.15848\n",
      "Iteration: 7750 Current cost: 0.166573\n",
      "Iteration: 7800 Current cost: 0.193414\n",
      "Iteration: 7850 Current cost: 0.150639\n",
      "Iteration: 7900 Current cost: 0.132238\n",
      "Iteration: 7950 Current cost: 0.131738\n",
      "Iteration: 8000 Current cost: 0.127599\n",
      "Iteration: 8050 Current cost: 0.132943\n",
      "Iteration: 8100 Current cost: 0.170707\n",
      "Iteration: 8150 Current cost: 0.179491\n",
      "Iteration: 8200 Current cost: 0.137361\n",
      "Iteration: 8250 Current cost: 0.148794\n",
      "Iteration: 8300 Current cost: 0.189893\n",
      "Iteration: 8350 Current cost: 0.163499\n",
      "Iteration: 8400 Current cost: 0.121786\n",
      "Iteration: 8450 Current cost: 0.194425\n",
      "Iteration: 8500 Current cost: 0.177975\n",
      "Iteration: 8550 Current cost: 0.141104\n",
      "Iteration: 8600 Current cost: 0.110942\n",
      "Iteration: 8650 Current cost: 0.150771\n",
      "Iteration: 8700 Current cost: 0.144082\n",
      "Iteration: 8750 Current cost: 0.171624\n",
      "Iteration: 8800 Current cost: 0.120642\n",
      "Iteration: 8850 Current cost: 0.122143\n",
      "Iteration: 8900 Current cost: 0.157973\n",
      "Iteration: 8950 Current cost: 0.11149\n",
      "Iteration: 9000 Current cost: 0.142821\n",
      "Iteration: 9050 Current cost: 0.18849\n",
      "Iteration: 9100 Current cost: 0.12266\n",
      "Iteration: 9150 Current cost: 0.132874\n",
      "Iteration: 9200 Current cost: 0.202219\n",
      "Iteration: 9250 Current cost: 0.134108\n",
      "Iteration: 9300 Current cost: 0.158281\n",
      "Iteration: 9350 Current cost: 0.106392\n",
      "Iteration: 9400 Current cost: 0.13591\n",
      "Iteration: 9450 Current cost: 0.163093\n",
      "Iteration: 9500 Current cost: 0.152412\n",
      "Iteration: 9550 Current cost: 0.205411\n",
      "Iteration: 9600 Current cost: 0.150529\n",
      "Iteration: 9650 Current cost: 0.125051\n",
      "Iteration: 9700 Current cost: 0.185617\n",
      "Iteration: 9750 Current cost: 0.177677\n",
      "Iteration: 9800 Current cost: 0.177414\n",
      "Iteration: 9850 Current cost: 0.156299\n",
      "Iteration: 9900 Current cost: 0.247195\n",
      "Iteration: 9950 Current cost: 0.19361\n",
      "Iteration: 10000 Current cost: 0.139527\n",
      "===============================================================================================\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "# use the above defined method to start the training:\n",
    "execute_graph(train_X, train_Y, model1, model_name, no_of_itreations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the accuracy on the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(dataX, dataY, exec_graph, model_name, threshold = 0.5):\n",
    "    '''\n",
    "        Function to run the trained model and calculate it's accuracy on the given inputs\n",
    "        @param \n",
    "        dataX, dataY => The data to be used for accuracy calculation\n",
    "        exec_graph => the Computation graph to be used\n",
    "        model_name => the model to restore the weights from\n",
    "        threshold => the accuracy threshold (by default it is 0.5)\n",
    "        @return => None (function has side effect)\n",
    "    '''\n",
    "    assert dataX.shape[-1] == dataY.shape[-1], \"The Dimensions of input X and labels Y don't match\"\n",
    "    \n",
    "    # the number of examples in the dataset\n",
    "    no_of_examples = dataX.shape[-1]\n",
    "    \n",
    "    with tf.Session(graph=exec_graph) as sess:\n",
    "        \n",
    "        # The saver object for saving and loading the model\n",
    "        saver = tf.train.Saver(max_to_keep=2)\n",
    "        \n",
    "        # the model must exist and you must be able to restore the weights\n",
    "        model_path = os.path.join(base_model_path, model_name)\n",
    "        assert os.path.isfile(os.path.join(model_path, \"checkpoint\")), \"Model doesn't exist\"\n",
    "        \n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "        \n",
    "        # compute the predictions given out by model\n",
    "        preds = sess.run(prediction, feed_dict={input_X: dataX.T, labels_Y: dataY.T})\n",
    "        \n",
    "        encoded_preds = (preds >= threshold).astype(np.float32)\n",
    "        \n",
    "        # calculate the accuracy in percentage:\n",
    "        correct = np.sum((encoded_preds == dataY.T).astype(np.int32))\n",
    "        accuracy = (float(correct) / dataX.shape[-1]) * 100 # for percentage\n",
    "        \n",
    "    # return the so calculated accuracy:\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../Models/Model1/Model1-10000\n",
      "Train_Set Accuracy: 96.3577883519\n"
     ]
    }
   ],
   "source": [
    "print \"Train_Set Accuracy: \" + str(calc_accuracy(train_X, train_Y, model1, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../Models/Model1/Model1-10000\n",
      "Dev Set Accuracy: 96.3038876382\n"
     ]
    }
   ],
   "source": [
    "print \"Dev Set Accuracy: \" + str(calc_accuracy(dev_X, dev_Y, model1, model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 works pretty cool. But I will experiment more with this model to see what else I can do to get better accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
