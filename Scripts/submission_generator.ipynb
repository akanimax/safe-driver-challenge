{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for generating the submission csv.\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "# Technology used: Tensorflow core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start with the usual utility cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# packages used for processing:\n",
    "import cPickle as pickle # for pickling the processed data\n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import numpy as np # numerical computations\n",
    "\n",
    "# for operating system related stuff\n",
    "import os\n",
    "import sys # for memory usage of objects\n",
    "from subprocess import check_output\n",
    "\n",
    "# pandas for extracting data from csv file\n",
    "import pandas as pd\n",
    "\n",
    "# the boss of deep learning frameworks\n",
    "import tensorflow as tf\n",
    "\n",
    "# to plot the images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "\n",
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print(check_output(cmd).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "LICENSE\n",
      "Models\n",
      "README.md\n",
      "Scripts\n",
      "submissions :D\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the structure of the project directory\n",
    "exec_command(['ls', '..'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Set the constants for the script '''\n",
    "\n",
    "# various paths of the files\n",
    "data_path = \"../Data\" # the data path\n",
    "base_model_path = \"../Models\"\n",
    "\n",
    "data_files = {\n",
    "    \"train\": os.path.join(data_path, \"train.csv\"),\n",
    "    \"test\": os.path.join(data_path, \"test.csv\")\n",
    "}\n",
    "\n",
    "base_model_path = '../Models'\n",
    "\n",
    "plug_and_play_data_file_path = os.path.join(data_path, \"plug_and_play_for_ANN.pickle\")\n",
    "\n",
    "# constants:\n",
    "(train_size, dev_size, test_size) = (0.9, 0.05, 0.05) # values are unit ratios\n",
    "no_of_features = 57\n",
    "no_of_itreations = 10000 \n",
    "batch_size = 512\n",
    "checkpoint_factor = 50\n",
    "lr = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to unpickle the given file and load the obj back into the python environment\n",
    "def unPickleIt(pickle_path): # might throw the file not found exception\n",
    "    '''\n",
    "        function to unpickle the object from the given path\n",
    "        @param\n",
    "        pickle_path => the path where the pickle file is located\n",
    "        @return => the object extracted from the saved path\n",
    "    '''\n",
    "\n",
    "    with open(pickle_path, 'rb') as dumped_pickle:\n",
    "        obj = pickle.load(dumped_pickle)\n",
    "\n",
    "    return obj # return the unpickled object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data from the test.csv file to generate predictions from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the means and variances from the plug_and_play file\n",
    "dat_dict = unPickleIt(plug_and_play_data_file_path)\n",
    "variances = dat_dict['variances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the dat_dict to free up resources\n",
    "del dat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57, 1)\n"
     ]
    }
   ],
   "source": [
    "# check the shapes of theses two vals\n",
    "print variances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the data from the test.csv file\n",
    "raw_data = pd.read_csv(data_files['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <th>...</th>\n",
       "      <th>ps_calc_11</th>\n",
       "      <th>ps_calc_12</th>\n",
       "      <th>ps_calc_13</th>\n",
       "      <th>ps_calc_14</th>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  ps_ind_01  ps_ind_02_cat  ps_ind_03  ps_ind_04_cat  ps_ind_05_cat  \\\n",
       "0   0          0              1          8              1              0   \n",
       "1   1          4              2          5              1              0   \n",
       "2   2          5              1          3              0              0   \n",
       "3   3          0              1          6              0              0   \n",
       "4   4          5              1          7              0              0   \n",
       "5   5          0              1          6              0              0   \n",
       "6   6          0              1          3              0              0   \n",
       "7   8          0              1          0              0              0   \n",
       "8  10          0              1          7              0              0   \n",
       "9  11          1              1          6              0              0   \n",
       "\n",
       "   ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  ps_ind_09_bin       ...        \\\n",
       "0              0              1              0              0       ...         \n",
       "1              0              0              0              1       ...         \n",
       "2              0              0              0              1       ...         \n",
       "3              1              0              0              0       ...         \n",
       "4              0              0              0              1       ...         \n",
       "5              1              0              0              0       ...         \n",
       "6              0              1              0              0       ...         \n",
       "7              1              0              0              0       ...         \n",
       "8              0              1              0              0       ...         \n",
       "9              0              0              0              1       ...         \n",
       "\n",
       "   ps_calc_11  ps_calc_12  ps_calc_13  ps_calc_14  ps_calc_15_bin  \\\n",
       "0           1           1           1          12               0   \n",
       "1           2           0           3          10               0   \n",
       "2           4           0           2           4               0   \n",
       "3           5           1           0           5               1   \n",
       "4           4           0           0           4               0   \n",
       "5           8           1           4           9               1   \n",
       "6           2           0           4           6               1   \n",
       "7           3           1           4           9               0   \n",
       "8           5           1           4           6               0   \n",
       "9           6           1           6          10               0   \n",
       "\n",
       "   ps_calc_16_bin  ps_calc_17_bin  ps_calc_18_bin  ps_calc_19_bin  \\\n",
       "0               1               1               0               0   \n",
       "1               0               1               1               0   \n",
       "2               0               0               0               0   \n",
       "3               0               1               0               0   \n",
       "4               1               1               0               0   \n",
       "5               0               1               0               1   \n",
       "6               1               0               0               0   \n",
       "7               1               0               0               0   \n",
       "8               0               1               0               0   \n",
       "9               1               1               0               0   \n",
       "\n",
       "   ps_calc_20_bin  \n",
       "0               1  \n",
       "1               1  \n",
       "2               0  \n",
       "3               0  \n",
       "4               1  \n",
       "5               0  \n",
       "6               0  \n",
       "7               0  \n",
       "8               0  \n",
       "9               0  \n",
       "\n",
       "[10 rows x 58 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a few rows of the raw_data\n",
    "raw_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note that the target column is missing! since these are the vals for which predictions are to be given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test examples to be predicted: 892816\n"
     ]
    }
   ],
   "source": [
    "# check the number of test examples for which predictions are to be generated.\n",
    "n_test_examples = raw_data['id'].count()\n",
    "print \"Total test examples to be predicted: \" + str(n_test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform this new data to normalize it using the earlier means and variances\n",
    "def normalize_data_frame(data, variances):\n",
    "    '''\n",
    "        function to normalize the pandas dataframe and convert it into a numpy array\n",
    "        @param\n",
    "        data => the pandas dataframe\n",
    "        variances => the variances array for variance correction\n",
    "        @return => features array\n",
    "    '''\n",
    "    \n",
    "    # create an empty data structure to hold all the data\n",
    "    features = np.ndarray(shape = (len(data.columns) - 1, data.id.count()))\n",
    "    \n",
    "    # iterate over all the columns and insert their slices into the features array after normalizing them\n",
    "    count = 0; # start the counter from 0 and perform the required stuff\n",
    "    for column in data.columns[1:]:\n",
    "        feature_slice = np.array(data[column]).reshape(1, -1) # carve out the feature slice\n",
    "        variance = variances[count]\n",
    "        \n",
    "        feature_slice = feature_slice + 1 # change of origin\n",
    "        feature_slice = feature_slice / variance # variance normalization\n",
    "        \n",
    "        # add the slice to the features vector\n",
    "        features[count, :] = feature_slice\n",
    "        \n",
    "        # do not forget to increment the counter\n",
    "        count += 1\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = normalize_data_frame(raw_data, variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test data: (57, 892816)\n"
     ]
    }
   ],
   "source": [
    "print \"Shape of test data: \" + str(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.54102959e-01,   1.27051480e+00],\n",
       "       [  4.52811810e+00,   6.79217715e+00],\n",
       "       [  1.23465964e+00,   8.23106425e-01],\n",
       "       [  8.21842224e+00,   8.21842224e+00],\n",
       "       [  5.48176031e-01,   5.48176031e-01],\n",
       "       [  4.18919668e+00,   4.18919668e+00],\n",
       "       [  1.04730173e+01,   5.23650864e+00],\n",
       "       [  7.29654360e+00,   7.29654360e+00],\n",
       "       [  6.62399692e+00,   1.32479938e+01],\n",
       "       [  2.68213551e+03,   2.68213551e+03]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[:10, :2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data has been properly set up. I can now proceed further with the predictions generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the num_units in each layer of the feed_forward neural network\n",
    "layer_dims = [512, 512, 512, 512, 512, 512, 512, 512, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "# scoped as Inputs\n",
    "with tf.variable_scope(\"Input\"):\n",
    "   # define the placeholders for the input data\n",
    "   # placeholder for feeding in input data batch\n",
    "   input_X = tf.placeholder(tf.float32, shape=(None, no_of_features), name=\"Input_features\")\n",
    "   labels_Y = tf.placeholder(tf.int32, shape=(None,), name=\"Ideal_labels\") # placeholder for the labels\n",
    "   one_hot_encoded_labels_Y = tf.one_hot(labels_Y, depth=2, axis=1, name=\"One_hot_label_encoder\")\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "one_hot_encoded_labels_Y\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "# scoped as model:\n",
    "with tf.variable_scope(\"Deep_Neural_Network\"):\n",
    "    # define the layers for the neural network.\n",
    "    with tf.name_scope(\"Encoder\"):\n",
    "        ''' This is The forward-backward neural network with abs activation function '''\n",
    "        # layer 1 => \n",
    "        fwd_lay1 = tf.layers.dense(input_X, layer_dims[0], activation=tf.abs, name=\"layer_1\")\n",
    "        # layer 2 =>\n",
    "        fwd_lay2 = tf.layers.dense(fwd_lay1, layer_dims[1], activation=tf.abs, name=\"layer_2\")\n",
    "        # layer 3 =>\n",
    "        fwd_lay3 = tf.layers.dense(fwd_lay2, layer_dims[2], activation=tf.abs, name=\"layer_3\")\n",
    "        # layer 4 =>\n",
    "        fwd_lay4 = tf.layers.dense(fwd_lay3, layer_dims[3], activation=tf.abs, name=\"layer_4\")\n",
    "        # layer 5 =>\n",
    "        fwd_lay5 = tf.layers.dense(fwd_lay4, layer_dims[4], activation=tf.abs, name=\"layer_5\")\n",
    "        # layer 6 =>\n",
    "        fwd_lay6 = tf.layers.dense(fwd_lay5, layer_dims[5], activation=tf.abs, name=\"layer_6\")\n",
    "        # layer 7 =>\n",
    "        fwd_lay7 = tf.layers.dense(fwd_lay6, layer_dims[6], activation=tf.abs, name=\"layer_7\")\n",
    "        # layer 8 =>\n",
    "        fwd_lay8 = tf.layers.dense(fwd_lay7, layer_dims[7], activation=tf.abs, name=\"layer_8\")\n",
    "        # layer 9 =>\n",
    "        fwd_lay9 = tf.layers.dense(fwd_lay8, layer_dims[8], activation=tf.abs, name=\"layer_9\")\n",
    "        \n",
    "    ''' Separately record all the activations as histograms '''\n",
    "    # recording the summaries to visualize separately\n",
    "    fwd_lay1_summary = tf.summary.histogram(\"fwd_lay1_summary\", fwd_lay1)\n",
    "    fwd_lay2_summary = tf.summary.histogram(\"fwd_lay2_summary\", fwd_lay2)\n",
    "    fwd_lay3_summary = tf.summary.histogram(\"fwd_lay3_summary\", fwd_lay3)\n",
    "    fwd_lay4_summary = tf.summary.histogram(\"fwd_lay4_summary\", fwd_lay4)\n",
    "    fwd_lay5_summary = tf.summary.histogram(\"fwd_lay5_summary\", fwd_lay5)\n",
    "    fwd_lay6_summary = tf.summary.histogram(\"fwd_lay6_summary\", fwd_lay6)\n",
    "    fwd_lay7_summary = tf.summary.histogram(\"fwd_lay7_summary\", fwd_lay7)\n",
    "    fwd_lay8_summary = tf.summary.histogram(\"fwd_lay8_summary\", fwd_lay8)\n",
    "    fwd_lay9_summary = tf.summary.histogram(\"fwd_lay9_summary\", fwd_lay9)\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"\", reuse=True):\n",
    "    # bring out all the weights from the network\n",
    "    lay_1_wts = tf.get_variable(\"Deep_Neural_Network/layer_1/kernel\")\n",
    "    lay_2_wts = tf.get_variable(\"Deep_Neural_Network/layer_2/kernel\")\n",
    "    lay_3_wts = tf.get_variable(\"Deep_Neural_Network/layer_3/kernel\")\n",
    "    lay_4_wts = tf.get_variable(\"Deep_Neural_Network/layer_4/kernel\")\n",
    "    lay_5_wts = tf.get_variable(\"Deep_Neural_Network/layer_5/kernel\")\n",
    "    lay_6_wts = tf.get_variable(\"Deep_Neural_Network/layer_6/kernel\")\n",
    "    lay_7_wts = tf.get_variable(\"Deep_Neural_Network/layer_7/kernel\")\n",
    "    lay_8_wts = tf.get_variable(\"Deep_Neural_Network/layer_8/kernel\")\n",
    "    lay_9_wts = tf.get_variable(\"Deep_Neural_Network/layer_9/kernel\")\n",
    "    \n",
    "    lay_1_biases = tf.get_variable(\"Deep_Neural_Network/layer_1/bias\")\n",
    "    lay_2_biases = tf.get_variable(\"Deep_Neural_Network/layer_2/bias\")\n",
    "    lay_3_biases = tf.get_variable(\"Deep_Neural_Network/layer_3/bias\")\n",
    "    lay_4_biases = tf.get_variable(\"Deep_Neural_Network/layer_4/bias\")\n",
    "    lay_5_biases = tf.get_variable(\"Deep_Neural_Network/layer_5/bias\")\n",
    "    lay_6_biases = tf.get_variable(\"Deep_Neural_Network/layer_6/bias\")\n",
    "    lay_7_biases = tf.get_variable(\"Deep_Neural_Network/layer_7/bias\")\n",
    "    lay_8_biases = tf.get_variable(\"Deep_Neural_Network/layer_8/bias\")\n",
    "    lay_9_biases = tf.get_variable(\"Deep_Neural_Network/layer_9/bias\")\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "lay_1_wts, lay_8_wts, lay_9_wts, lay_1_biases, lay_8_biases, lay_9_biases\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "y_back_in = fwd_lay9\n",
    "y_back_in\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Decoder\"):\n",
    "        lay_0_biases = tf.get_variable(\"layer_0/bias\", shape=(no_of_features, ))\n",
    "    \n",
    "        # layer 1 => \n",
    "        bwd_lay1 = tf.abs(tf.matmul(y_back_in, tf.transpose(lay_9_wts)) + lay_8_biases)\n",
    "        # layer 2 => \n",
    "        bwd_lay2 = tf.abs(tf.matmul(bwd_lay1, tf.transpose(lay_8_wts)) + lay_7_biases)\n",
    "        # layer 3 => \n",
    "        bwd_lay3 = tf.abs(tf.matmul(bwd_lay2, tf.transpose(lay_7_wts)) + lay_6_biases)\n",
    "        # layer 4 => \n",
    "        bwd_lay4 = tf.abs(tf.matmul(bwd_lay3, tf.transpose(lay_6_wts)) + lay_5_biases)\n",
    "        # layer 5 => \n",
    "        bwd_lay5 = tf.abs(tf.matmul(bwd_lay4, tf.transpose(lay_5_wts)) + lay_4_biases)\n",
    "        # layer 6 => \n",
    "        bwd_lay6 = tf.abs(tf.matmul(bwd_lay5, tf.transpose(lay_4_wts)) + lay_3_biases)\n",
    "        # layer 7 => \n",
    "        bwd_lay7 = tf.abs(tf.matmul(bwd_lay6, tf.transpose(lay_3_wts)) + lay_2_biases)\n",
    "        # layer 8 => \n",
    "        bwd_lay8 = tf.abs(tf.matmul(bwd_lay7, tf.transpose(lay_2_wts)) + lay_1_biases)\n",
    "        # layer 9 => \n",
    "        bwd_lay9 = tf.abs(tf.matmul(bwd_lay8, tf.transpose(lay_1_wts)) + lay_0_biases)\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "x_back_out = bwd_lay9\n",
    "x_back_out, input_X\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "# function to compute the directional cosines of the input values\n",
    "def directional_cosines(X):\n",
    "    ''' \n",
    "        calculate the directional cosines of the inputs\n",
    "    '''\n",
    "    square = tf.square(X)\n",
    "    sum_square = tf.reduce_sum(square, axis=1, keep_dims=True)\n",
    "    dcs = X / tf.sqrt(sum_square)\n",
    "    \n",
    "    # return the directional cosines:\n",
    "    return dcs\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "# scoped as predictions\n",
    "with tf.variable_scope(\"Prediction\"):\n",
    "    prediction = directional_cosines(y_back_in)\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "# scoped as loss\n",
    "with tf.variable_scope(\"Loss\"):\n",
    "    # define the forward loss\n",
    "    fwd_loss = tf.reduce_mean(tf.abs(prediction - one_hot_encoded_labels_Y))\n",
    "    \n",
    "    # define the reverse loss\n",
    "    rev_loss = tf.reduce_mean(tf.abs(x_back_out - input_X))\n",
    "    \n",
    "    total_loss = fwd_loss + rev_loss\n",
    "        \n",
    "    # record the loss summary:\n",
    "    tf.summary.scalar(\"Fwd_loss\", fwd_loss)\n",
    "    tf.summary.scalar(\"Bwd_loss\", rev_loss)\n",
    "    tf.summary.scalar(\"Tot_loss\", total_loss)\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "# scoped as train_step\n",
    "with tf.variable_scope(\"Train_Step\"):\n",
    "    # define the optimizer and the train_step:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr) # this has been manually tuned\n",
    "    train_step = optimizer.minimize(total_loss, name=\"train_step\")\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "# scoped as init operation\n",
    "with tf.variable_scope(\"Init\"):\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "# scoped as summaries\n",
    "with tf.variable_scope(\"Summary\"):\n",
    "    all_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_predictions(dataX, exec_graph, model_name):\n",
    "    '''\n",
    "        Function to run the trained model and generate predictions for the given data\n",
    "        @param \n",
    "        dataX => The data to be used for accuracy calculation\n",
    "        exec_graph => the Computation graph to be used\n",
    "        model_name => the model to restore the weights from\n",
    "        @return => predictions array returned\n",
    "    '''\n",
    "    \n",
    "    # the number of examples in the dataset\n",
    "    no_of_examples = dataX.shape[-1]\n",
    "    \n",
    "    with tf.Session(graph=exec_graph) as sess:\n",
    "        \n",
    "        # The saver object for saving and loading the model\n",
    "        saver = tf.train.Saver(max_to_keep=2)\n",
    "        \n",
    "        # the model must exist and you must be able to restore the weights\n",
    "        model_path = os.path.join(base_model_path, model_name)\n",
    "        assert os.path.isfile(os.path.join(model_path, \"checkpoint\")), \"Model doesn't exist\"\n",
    "        \n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "        \n",
    "        # compute the predictions given out by model\n",
    "        preds = sess.run(prediction, feed_dict={input_X: dataX.T})\n",
    "        \n",
    "    # return the so calculated accuracy:\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../Models/../Models/Model2/Model2-250\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "    WARNING! WARNING! WARNING!\n",
    "    Keep an eye on the htop meter while executing this cell. The machine might freeze momentarily if it\n",
    "    is a low end machine.\n",
    "'''\n",
    "\n",
    "# get the predictions for the test_data.\n",
    "model_name = os.path.join(base_model_path, \"Model2\")\n",
    "predictions = generate_predictions(test_data, tf.get_default_graph(), model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the sample_submission.csv file and extract the ids from it.\n",
    "sample_submission = pd.read_csv(os.path.join(data_path, \"sample_submission.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.array(sample_submission['id'])\n",
    "print \"The ids column has a shape: \" + str(ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the shape of the generated predictions\n",
    "print \"The generated predictions have the shape: \" + str(np.squeeze(predictions).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets quickly write the function to generate the subimssion csv file\n",
    "def generate_submission_file(preds, save_path, model_name):\n",
    "    '''\n",
    "        function to generate the submission file. \n",
    "        @param\n",
    "        preds => the predictions to be written to the file\n",
    "        model_name => the model used for this generation \n",
    "        save_path => the path where the file needs to be saved\n",
    "        @return => None (check the save path where the file is saved)\n",
    "    '''\n",
    "    save_file = save_path + '_' + model_name + '.csv'\n",
    "    \n",
    "    with open(save_file, 'w') as submission:\n",
    "        # write the header to the file\n",
    "        submission.write(\"id,target\\n\")\n",
    "        for (idx, prediction) in zip(ids, np.squeeze(predictions)):\n",
    "            line = str(idx) + ',' + str(prediction) + '\\n'\n",
    "            submission.write(line) # write the line to the file\n",
    "    \n",
    "    # print a feedback statement to notify the required file generation\n",
    "    print \"The file has been generated at: \" + save_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the above function to generate the submission file\n",
    "generate_submission_file(predictions, os.path.join(data_path, \"submission\"), \"Model1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
