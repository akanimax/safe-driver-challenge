{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for generating the submission csv.\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "# Technology used: Tensorflow core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start with the usual utility cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# packages used for processing:\n",
    "import cPickle as pickle # for pickling the processed data\n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import numpy as np # numerical computations\n",
    "\n",
    "# for operating system related stuff\n",
    "import os\n",
    "import sys # for memory usage of objects\n",
    "from subprocess import check_output\n",
    "\n",
    "# pandas for extracting data from csv file\n",
    "import pandas as pd\n",
    "\n",
    "# the boss of deep learning frameworks\n",
    "import tensorflow as tf\n",
    "\n",
    "# to plot the images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply the seaborn makeup on the plots drawn using matplotlib\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "\n",
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print(check_output(cmd).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "LICENSE\n",
      "Models\n",
      "README.md\n",
      "Scripts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the structure of the project directory\n",
    "exec_command(['ls', '..'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Set the constants for the script '''\n",
    "\n",
    "# various paths of the files\n",
    "data_path = \"../Data\" # the data path\n",
    "base_model_path = \"../Models\"\n",
    "\n",
    "data_files = {\n",
    "    \"train\": os.path.join(data_path, \"train.csv\"),\n",
    "    \"test\": os.path.join(data_path, \"test.csv\")\n",
    "}\n",
    "\n",
    "base_model_path = '../Models'\n",
    "\n",
    "plug_and_play_data_file_path = os.path.join(data_path, \"plug_and_play.pickle\")\n",
    "\n",
    "# constants:\n",
    "(train_size, dev_size, test_size) = (0.9, 0.05, 0.05) # values are unit ratios\n",
    "no_of_features = 57\n",
    "no_of_itreations = 10000 \n",
    "batch_size = 512\n",
    "checkpoint_factor = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to unpickle the given file and load the obj back into the python environment\n",
    "def unPickleIt(pickle_path): # might throw the file not found exception\n",
    "    '''\n",
    "        function to unpickle the object from the given path\n",
    "        @param\n",
    "        pickle_path => the path where the pickle file is located\n",
    "        @return => the object extracted from the saved path\n",
    "    '''\n",
    "\n",
    "    with open(pickle_path, 'rb') as dumped_pickle:\n",
    "        obj = pickle.load(dumped_pickle)\n",
    "\n",
    "    return obj # return the unpickled object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data from the test.csv file to generate predictions from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the means and variances from the plug_and_play file\n",
    "dat_dict = unPickleIt(plug_and_play_data_file_path)\n",
    "means = dat_dict['means']; variances = dat_dict['variances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57, 1) (57, 1)\n"
     ]
    }
   ],
   "source": [
    "# check the shapes of theses two vals\n",
    "print means.shape, variances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can delete the dat_dict now. To free up resources\n",
    "del dat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the data from the test.csv file\n",
    "raw_data = pd.read_csv(data_files['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <th>...</th>\n",
       "      <th>ps_calc_11</th>\n",
       "      <th>ps_calc_12</th>\n",
       "      <th>ps_calc_13</th>\n",
       "      <th>ps_calc_14</th>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  ps_ind_01  ps_ind_02_cat  ps_ind_03  ps_ind_04_cat  ps_ind_05_cat  \\\n",
       "0   0          0              1          8              1              0   \n",
       "1   1          4              2          5              1              0   \n",
       "2   2          5              1          3              0              0   \n",
       "3   3          0              1          6              0              0   \n",
       "4   4          5              1          7              0              0   \n",
       "5   5          0              1          6              0              0   \n",
       "6   6          0              1          3              0              0   \n",
       "7   8          0              1          0              0              0   \n",
       "8  10          0              1          7              0              0   \n",
       "9  11          1              1          6              0              0   \n",
       "\n",
       "   ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  ps_ind_09_bin       ...        \\\n",
       "0              0              1              0              0       ...         \n",
       "1              0              0              0              1       ...         \n",
       "2              0              0              0              1       ...         \n",
       "3              1              0              0              0       ...         \n",
       "4              0              0              0              1       ...         \n",
       "5              1              0              0              0       ...         \n",
       "6              0              1              0              0       ...         \n",
       "7              1              0              0              0       ...         \n",
       "8              0              1              0              0       ...         \n",
       "9              0              0              0              1       ...         \n",
       "\n",
       "   ps_calc_11  ps_calc_12  ps_calc_13  ps_calc_14  ps_calc_15_bin  \\\n",
       "0           1           1           1          12               0   \n",
       "1           2           0           3          10               0   \n",
       "2           4           0           2           4               0   \n",
       "3           5           1           0           5               1   \n",
       "4           4           0           0           4               0   \n",
       "5           8           1           4           9               1   \n",
       "6           2           0           4           6               1   \n",
       "7           3           1           4           9               0   \n",
       "8           5           1           4           6               0   \n",
       "9           6           1           6          10               0   \n",
       "\n",
       "   ps_calc_16_bin  ps_calc_17_bin  ps_calc_18_bin  ps_calc_19_bin  \\\n",
       "0               1               1               0               0   \n",
       "1               0               1               1               0   \n",
       "2               0               0               0               0   \n",
       "3               0               1               0               0   \n",
       "4               1               1               0               0   \n",
       "5               0               1               0               1   \n",
       "6               1               0               0               0   \n",
       "7               1               0               0               0   \n",
       "8               0               1               0               0   \n",
       "9               1               1               0               0   \n",
       "\n",
       "   ps_calc_20_bin  \n",
       "0               1  \n",
       "1               1  \n",
       "2               0  \n",
       "3               0  \n",
       "4               1  \n",
       "5               0  \n",
       "6               0  \n",
       "7               0  \n",
       "8               0  \n",
       "9               0  \n",
       "\n",
       "[10 rows x 58 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a few rows of the raw_data\n",
    "raw_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note that the target column is missing! since these are the vals for which predictions are to be given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test examples to be predicted: 892816\n"
     ]
    }
   ],
   "source": [
    "# check the number of test examples for which predictions are to be generated.\n",
    "n_test_examples = raw_data['id'].count()\n",
    "print \"Total test examples to be predicted: \" + str(n_test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform this new data to normalize it using the earlier means and variances\n",
    "def normalize_data_frame(data, means, variances):\n",
    "    '''\n",
    "        function to normalize the pandas dataframe and convert it into a numpy array\n",
    "        @param\n",
    "        data => the pandas dataframe\n",
    "        means => the means array for mean cancellation\n",
    "        variances => the variances array for variance correction\n",
    "        @return => features array\n",
    "    '''\n",
    "    \n",
    "    # create an empty data structure to hold all the data\n",
    "    features = np.ndarray(shape = (len(data.columns) - 1, data.id.count()))\n",
    "    \n",
    "    # iterate over all the columns and insert their slices into the features array after normalizing them\n",
    "    count = 0; # start the counter from 0 and perform the required stuff\n",
    "    for column in data.columns[1:]:\n",
    "        feature_slice = np.array(data[column]).reshape(1, -1) # carve out the feature slice\n",
    "        mean = means[count]\n",
    "        variance = variances[count]\n",
    "        \n",
    "        feature_slice = feature_slice - mean # mean cancellation\n",
    "        feature_slice = feature_slice / variance # variance normalization\n",
    "        \n",
    "        # add the slice to the features vector\n",
    "        features[count, :] = feature_slice\n",
    "        \n",
    "        # do not forget to increment the counter\n",
    "        count += 1\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = normalize_data_frame(raw_data, means, variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test data: (57, 892816)\n"
     ]
    }
   ],
   "source": [
    "print \"Shape of test data: \" + str(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -4.82890952e-01,   5.33519177e-01],\n",
       "       [ -8.12666095e-01,   1.45138915e+00],\n",
       "       [  4.90664154e-01,   7.91116329e-02],\n",
       "       [  2.39651111e+00,   2.39651111e+00],\n",
       "       [ -2.22114198e-01,  -2.22114198e-01],\n",
       "       [ -1.64946017e+00,  -1.64946017e+00],\n",
       "       [  3.89054769e+00,  -1.34595216e+00],\n",
       "       [ -1.19605782e+00,  -1.19605782e+00],\n",
       "       [ -1.22744923e+00,   5.39653656e+00],\n",
       "       [ -1.00037143e+00,  -1.00037143e+00],\n",
       "       [ -1.00169302e+00,  -1.00169302e+00],\n",
       "       [ -1.00952689e+00,  -1.00952689e+00],\n",
       "       [ -1.00094678e+00,  -1.00094678e+00],\n",
       "       [ -7.65382390e-01,  -7.65382390e-01],\n",
       "       [  3.73781082e-01,  -1.82904874e-01],\n",
       "       [  1.51326113e+00,   1.51326113e+00],\n",
       "       [ -1.13775966e+00,  -1.13775966e+00],\n",
       "       [ -1.18125780e+00,  -1.18125780e+00],\n",
       "       [ -1.34147559e+00,   3.49304620e+00],\n",
       "       [ -8.51647113e-01,   3.72121314e-01],\n",
       "       [  9.40614751e-02,   3.49813546e-01],\n",
       "       [ -2.05984213e-01,  -6.82824094e-01],\n",
       "       [  1.20477375e+00,   1.20477375e+00],\n",
       "       [ -7.96013399e-01,  -7.96013399e-01],\n",
       "       [ -1.56378931e-01,  -1.56378931e-01],\n",
       "       [ -1.18123457e+00,   2.21210518e-01],\n",
       "       [ -1.83551148e-01,   1.46853742e-01],\n",
       "       [  7.46770480e-01,   7.46770480e-01],\n",
       "       [  1.20180550e+00,   1.20180550e+00],\n",
       "       [  7.00572433e-01,  -1.38722823e+00],\n",
       "       [  9.36912987e-01,   9.36912987e-01],\n",
       "       [  2.55484386e-03,   3.74229165e-02],\n",
       "       [ -1.94200263e+00,  -1.94200263e+00],\n",
       "       [ -1.87291057e+01,  -1.87291057e+01],\n",
       "       [ -2.84910387e+00,  -4.10280378e+00],\n",
       "       [  5.94862464e-01,   6.43413486e-01],\n",
       "       [  7.44447066e-01,  -4.43959334e-01],\n",
       "       [ -4.24035848e+00,  -6.03233947e-01],\n",
       "       [  4.25731802e+00,   6.12466075e-01],\n",
       "       [  1.82096733e+00,  -6.04544089e-01],\n",
       "       [ -1.09926686e+00,   5.03068514e-01],\n",
       "       [ -6.87767975e-01,   8.64955391e-01],\n",
       "       [ -9.48919040e-01,   1.74430913e-01],\n",
       "       [ -2.91012577e-03,   4.96842262e-01],\n",
       "       [ -1.51405261e+00,   3.63315633e-01],\n",
       "       [ -2.18044700e-01,  -2.18044700e-01],\n",
       "       [  6.71365501e-02,  -1.69923374e-01],\n",
       "       [ -8.16087284e-01,  -6.32341044e-01],\n",
       "       [ -3.05377942e-01,  -9.96406191e-01],\n",
       "       [ -6.51765121e-01,   4.44582033e-02],\n",
       "       [  5.91319892e-01,   3.26211892e-01],\n",
       "       [ -1.13950439e+00,  -1.13950439e+00],\n",
       "       [  1.59275939e+00,  -2.68701301e+00],\n",
       "       [  1.80445710e+00,   1.80445710e+00],\n",
       "       [ -1.40287972e+00,   3.48211005e+00],\n",
       "       [ -1.53615128e+00,  -1.53615128e+00],\n",
       "       [  6.52235993e+00,   6.52235993e+00]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[:, :2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data has been properly set up. I can now proceed further with the predictions generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_dims = [512, 512, 512, 256, 1] # the num_units in each layer of the feed_forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the tensorflow computation graph (THE MAIN NEURAL NETWORK):\n",
    "\n",
    "model1 = tf.Graph()\n",
    "\n",
    "with model1.as_default():\n",
    "    # scoped as Inputs\n",
    "    with tf.variable_scope(\"Input\"):\n",
    "        \n",
    "        # define the placeholders for the input data\n",
    "        input_X = tf.placeholder(tf.float32, shape=(None, no_of_features), name=\"Input_features\") # placeholder for feeding in input data batch\n",
    "        labels_Y = tf.placeholder(tf.float32, shape=(None, 1), name=\"Ideal_labels\") # placeholder for the labels\n",
    "    \n",
    "    # scoped as model:\n",
    "    with tf.variable_scope(\"Deep_Neural_Network\"):\n",
    "        \n",
    "        # define the layers for the neural network.\n",
    "        ''' This is a plain and simple neural network with relu activations '''\n",
    "        # layer 1 => \n",
    "        lay1 = tf.layers.dense(input_X, layer_dims[0], activation=tf.nn.relu, name=\"layer_1\")\n",
    "        # layer 2 =>\n",
    "        lay2 = tf.layers.dense(lay1, layer_dims[1], activation=tf.nn.relu, name=\"layer_2\")\n",
    "        # layer 3 =>\n",
    "        lay3 = tf.layers.dense(lay2, layer_dims[2], activation=tf.nn.relu, name=\"layer_3\")\n",
    "        # layer 4 =>\n",
    "        lay4 = tf.layers.dense(lay3, layer_dims[3], activation=tf.nn.relu, name=\"layer_4\")\n",
    "        # layer 5 =>\n",
    "        # the last layer has activation sigmoid since it is going to output probability.\n",
    "        lay5 = tf.layers.dense(lay4, layer_dims[4], name=\"output\") # the activation is linear\n",
    "        \n",
    "        \n",
    "        ''' Separately record all the activations as histograms '''\n",
    "        # recording the summaries to visualize separately\n",
    "        lay1_summary = tf.summary.histogram(\"lay1_summary\", lay1)\n",
    "        lay2_summary = tf.summary.histogram(\"lay2_summary\", lay2)\n",
    "        lay3_summary = tf.summary.histogram(\"lay3_summary\", lay3)\n",
    "        lay4_summary = tf.summary.histogram(\"lay4_summary\", lay4)\n",
    "        output_summary = tf.summary.histogram(\"output_summary\", lay5)\n",
    "        \n",
    "    # scoped as predictions\n",
    "    with tf.variable_scope(\"Prediction\"):\n",
    "        prediction = tf.nn.sigmoid(lay5, name=\"sigmoid\") # apply sigmoid to the linear activation of the output\n",
    "        \n",
    "    # scoped as loss\n",
    "    with tf.variable_scope(\"Loss\"):\n",
    "        \n",
    "        # define the loss function.\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=lay5, labels=labels_Y), name=\"loss\")\n",
    "        # we use the softmanx_cross_entropy_with_logits function for this.\n",
    "        \n",
    "        # record the loss summary:\n",
    "        tf.summary.scalar(\"Loss\", loss)\n",
    "        \n",
    "    # scoped as train_step\n",
    "    with tf.variable_scope(\"Train_Step\"):\n",
    "    \n",
    "        # define the optimizer and the train_step:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=1e-6) # use the default learning rate\n",
    "        train_step = optimizer.minimize(loss, name=\"train_step\")\n",
    "        \n",
    "    # scoped as init operation\n",
    "    with tf.variable_scope(\"Init\"):\n",
    "        init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    # scoped as summaries\n",
    "    with tf.variable_scope(\"Summary\"):\n",
    "        all_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_predictions(dataX, exec_graph, model_name):\n",
    "    '''\n",
    "        Function to run the trained model and generate predictions for the given data\n",
    "        @param \n",
    "        dataX => The data to be used for accuracy calculation\n",
    "        exec_graph => the Computation graph to be used\n",
    "        model_name => the model to restore the weights from\n",
    "        @return => predictions array returned\n",
    "    '''\n",
    "    \n",
    "    # the number of examples in the dataset\n",
    "    no_of_examples = dataX.shape[-1]\n",
    "    \n",
    "    with tf.Session(graph=exec_graph) as sess:\n",
    "        \n",
    "        # The saver object for saving and loading the model\n",
    "        saver = tf.train.Saver(max_to_keep=2)\n",
    "        \n",
    "        # the model must exist and you must be able to restore the weights\n",
    "        model_path = os.path.join(base_model_path, model_name)\n",
    "        assert os.path.isfile(os.path.join(model_path, \"checkpoint\")), \"Model doesn't exist\"\n",
    "        \n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "        \n",
    "        # compute the predictions given out by model\n",
    "        preds = sess.run(prediction, feed_dict={input_X: dataX.T})\n",
    "        \n",
    "    # return the so calculated accuracy:\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../Models/../Models/Model1/Model1-10000\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "    WARNING! WARNING! WARNING!\n",
    "    Keep an eye on the htop meter while executing this cell. The machine might freeze momentarily if it\n",
    "    is a low end machine.\n",
    "'''\n",
    "\n",
    "# get the predictions for the test_data.\n",
    "model_name = os.path.join(base_model_path, \"Model1\")\n",
    "predictions = generate_predictions(test_data, model1, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the sample_submission.csv file and extract the ids from it.\n",
    "sample_submission = pd.read_csv(os.path.join(data_path, \"sample_submission.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ids column has a shape: (892816,)\n"
     ]
    }
   ],
   "source": [
    "ids = np.array(sample_submission['id'])\n",
    "print \"The ids column has a shape: \" + str(ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated predictions have the shape: (892816,)\n"
     ]
    }
   ],
   "source": [
    "# print the shape of the generated predictions\n",
    "print \"The generated predictions have the shape: \" + str(np.squeeze(predictions).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets quickly write the function to generate the subimssion csv file\n",
    "def generate_submission_file(preds, save_path, model_name):\n",
    "    '''\n",
    "        function to generate the submission file. \n",
    "        @param\n",
    "        preds => the predictions to be written to the file\n",
    "        model_name => the model used for this generation \n",
    "        save_path => the path where the file needs to be saved\n",
    "        @return => None (check the save path where the file is saved)\n",
    "    '''\n",
    "    save_file = save_path + '_' + model_name + '.csv'\n",
    "    \n",
    "    with open(save_file, 'w') as submission:\n",
    "        # write the header to the file\n",
    "        submission.write(\"id,target\\n\")\n",
    "        for (idx, prediction) in zip(ids, np.squeeze(predictions)):\n",
    "            line = str(idx) + ',' + str(prediction) + '\\n'\n",
    "            submission.write(line) # write the line to the file\n",
    "    \n",
    "    # print a feedback statement to notify the required file generation\n",
    "    print \"The file has been generated at: \" + save_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has been generated at: ../Data/submission_Model1.csv\n"
     ]
    }
   ],
   "source": [
    "# use the above function to generate the submission file\n",
    "generate_submission_file(predictions, os.path.join(data_path, \"submission\"), \"Model1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
